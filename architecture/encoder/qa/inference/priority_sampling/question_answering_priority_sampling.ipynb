{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:09.433959Z",
          "iopub.status.busy": "2025-10-07T17:16:09.433691Z",
          "iopub.status.idle": "2025-10-07T17:16:09.458107Z",
          "shell.execute_reply": "2025-10-07T17:16:09.457366Z",
          "shell.execute_reply.started": "2025-10-07T17:16:09.433935Z"
        },
        "id": "NBGzobTPxf_u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional, Union\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, config, top_k: Optional[int] = 256):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_heads = config.n_heads\n",
        "        self.dim = config.dim\n",
        "        self.dropout = nn.Dropout(p=getattr(config, \"attention_dropout\", 0.0))\n",
        "        self.is_causal = False\n",
        "        self.attention_head_size = self.dim // self.n_heads\n",
        "        self.pruned_heads: set[int] = set()\n",
        "\n",
        "        if self.dim % self.n_heads != 0:\n",
        "            raise ValueError(f\"n_heads ({self.n_heads}) must divide dim ({self.dim})\")\n",
        "\n",
        "        self.q_lin = nn.Linear(self.dim, self.dim)\n",
        "        self.k_lin = nn.Linear(self.dim, self.dim)\n",
        "        self.v_lin = nn.Linear(self.dim, self.dim)\n",
        "        self.out_lin = nn.Linear(self.dim, self.dim)\n",
        "\n",
        "        self.threshold_top_k = top_k\n",
        "\n",
        "        # Register buffers for hash values\n",
        "        self.register_buffer('hash_values', None)\n",
        "\n",
        "    def prune_heads(self, heads: list[int]):\n",
        "        if not heads:\n",
        "            return\n",
        "\n",
        "        from transformers.modeling_utils import find_pruneable_heads_and_indices, prune_linear_layer\n",
        "\n",
        "        heads_to_prune, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.n_heads, self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
        "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
        "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
        "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
        "\n",
        "        self.n_heads -= len(heads_to_prune)\n",
        "        self.dim = self.attention_head_size * self.n_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads_to_prune)\n",
        "\n",
        "    def _prepare_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        bs, seq_len, _ = x.size()\n",
        "        return x.view(bs, seq_len, self.n_heads, self.attention_head_size).transpose(1, 2)\n",
        "\n",
        "    def _initialize_hash_values(self, bs: int, n_heads: int, k_len: int, device: torch.device):\n",
        "        \"\"\"Initialize hash values for threshold sampling.\"\"\"\n",
        "        if self.hash_values is None or self.hash_values.size(0) != bs or self.hash_values.size(1) != n_heads or self.hash_values.size(2) != k_len:\n",
        "            # Generate random hash values for each batch, head, and sequence position\n",
        "            self.hash_values = torch.rand(bs, n_heads, k_len, device=device)\n",
        "\n",
        "    def _get_threshold_sample_mask(self, k: torch.Tensor, top_k: Optional[int]) -> Optional[torch.Tensor]:\n",
        "        \"\"\"Threshold sampling as described in your algorithm.\"\"\"\n",
        "        if top_k is None:\n",
        "            return None\n",
        "\n",
        "        bs, n_heads, k_len, head_dim = k.shape\n",
        "        device = k.device\n",
        "\n",
        "        effective_top_k = top_k if top_k is not None else self.threshold_top_k\n",
        "        if effective_top_k is None:\n",
        "            return None\n",
        "\n",
        "        k_samples = min(int(effective_top_k), k_len)\n",
        "\n",
        "        if k_samples >= k_len:\n",
        "            return torch.ones(bs, n_heads, k_len, dtype=torch.bool, device=device)\n",
        "\n",
        "        self._initialize_hash_values(bs, n_heads, k_len, device)\n",
        "\n",
        "        batch_masks = []\n",
        "\n",
        "        for b in range(bs):\n",
        "            per_head_masks = []\n",
        "            for h in range(n_heads):\n",
        "                # Compute A_norm_sq (Frobenius norm squared of the key matrix for this head)\n",
        "                K_head = k[b, h]  # (k_len, head_dim)\n",
        "                A_norm_sq = torch.linalg.norm(K_head, 'fro') ** 2\n",
        "\n",
        "                # Compute threshold tau = k / A_norm_sq\n",
        "                tau = k_samples / A_norm_sq if A_norm_sq > 0 else float('inf')\n",
        "\n",
        "                selected_indices = []\n",
        "\n",
        "                for i in range(k_len):\n",
        "                    row_norm_sq = torch.linalg.norm(K_head[i]) ** 2\n",
        "                    if self.hash_values[b, h, i] <= tau * row_norm_sq:\n",
        "                        selected_indices.append(i)\n",
        "\n",
        "                # Create boolean mask\n",
        "                mask = torch.zeros(k_len, dtype=torch.bool, device=device)\n",
        "                mask[selected_indices] = True\n",
        "                per_head_masks.append(mask)\n",
        "\n",
        "            batch_masks.append(torch.stack(per_head_masks, dim=0))\n",
        "\n",
        "        return torch.stack(batch_masks, dim=0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "        threshold_set: Optional[Union[torch.BoolTensor, torch.LongTensor]] = None,\n",
        "        top_k: Optional[int] = None,\n",
        "    ):\n",
        "        bs, q_len, _ = query.size()\n",
        "        k_len = key.size(1)\n",
        "        head_dim = self.attention_head_size\n",
        "\n",
        "        q = self._prepare_heads(self.q_lin(query))\n",
        "        k = self._prepare_heads(self.k_lin(key))\n",
        "        v = self._prepare_heads(self.v_lin(value))\n",
        "\n",
        "        q = q / math.sqrt(head_dim)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "        # Handle masking\n",
        "        if mask is not None:\n",
        "            attention_mask = mask < 0\n",
        "        else:\n",
        "            attention_mask = torch.zeros(bs, 1, q_len, k_len, dtype=torch.bool, device=scores.device)\n",
        "\n",
        "        # Compute threshold sampling mask if required\n",
        "        if threshold_set is not None or top_k is not None:\n",
        "            if threshold_set is not None:\n",
        "                # Use provided threshold set\n",
        "                if threshold_set.dim() == 1:\n",
        "                    mask_keep = threshold_set.view(1, 1, 1, k_len).expand(bs, self.n_heads, q_len, k_len)\n",
        "                else:\n",
        "                    mask_keep = threshold_set.view(bs, 1, 1, k_len).expand(bs, self.n_heads, q_len, k_len)\n",
        "            else:\n",
        "                # Generate threshold sampling mask\n",
        "                mask_keep = self._get_threshold_sample_mask(k, top_k)\n",
        "                # Reshape to (bs, n_heads, 1, k_len) for broadcasting\n",
        "                mask_keep = mask_keep.unsqueeze(2).expand(bs, self.n_heads, q_len, k_len)\n",
        "\n",
        "            # Combine masks: mask out tokens that are either originally masked OR not in threshold set\n",
        "            attention_mask = attention_mask | ~mask_keep\n",
        "\n",
        "        # Apply the final combined mask\n",
        "        scores.masked_fill_(attention_mask, torch.finfo(scores.dtype).min)\n",
        "\n",
        "        weights = nn.functional.softmax(scores, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        if head_mask is not None:\n",
        "            weights = weights * head_mask\n",
        "\n",
        "        context = torch.matmul(weights, v)\n",
        "        context = context.transpose(1, 2).contiguous().view(bs, q_len, self.dim)\n",
        "        context = self.out_lin(context)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context, weights)\n",
        "\n",
        "        return (context,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:09.459888Z",
          "iopub.status.busy": "2025-10-07T17:16:09.459637Z",
          "iopub.status.idle": "2025-10-07T17:16:10.131851Z",
          "shell.execute_reply": "2025-10-07T17:16:10.131068Z",
          "shell.execute_reply.started": "2025-10-07T17:16:09.459863Z"
        },
        "id": "SPaYx2mJxf_w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Define the model checkpoint you want to use\n",
        "model_checkpoint = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "# Load the correct FAST tokenizer for the CASED model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load the model WITH THE QUESTION ANSWERING HEAD\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3iiRXC0xf_y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#mha with kev attention, remove this cell for vanilla\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "for i, layer in enumerate(model.distilbert.transformer.layer):\n",
        "    custom_attention = MultiHeadSelfAttention(model.config, top_k=32)\n",
        "    custom_attention.q_lin.load_state_dict(layer.attention.q_lin.state_dict())\n",
        "    custom_attention.k_lin.load_state_dict(layer.attention.k_lin.state_dict())\n",
        "    custom_attention.v_lin.load_state_dict(layer.attention.v_lin.state_dict())\n",
        "    custom_attention.out_lin.load_state_dict(layer.attention.out_lin.state_dict())\n",
        "    custom_attention.to(device)\n",
        "    layer.attention = custom_attention\n",
        "\n",
        "print(\"loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIjGaMnv868A"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"squad\")\n",
        "\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZgLO_o69TlG"
      },
      "outputs": [],
      "source": [
        "train_dataset = ds['train']\n",
        "validation_dataset = ds['validation']\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize the question and context together\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Store the sample mapping BEFORE popping it\n",
        "    sample_map = inputs[\"overflow_to_sample_mapping\"]\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    inputs.pop(\"overflow_to_sample_mapping\")  # Remove from main inputs but we keep sample_map\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]  # Use the stored sample_map\n",
        "        answer = examples[\"answers\"][sample_idx]\n",
        "\n",
        "        # Get the character start and end of the answer\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        context_start = 0\n",
        "        context_end = len(sequence_ids) - 1\n",
        "\n",
        "        # Find where the context actually starts and ends\n",
        "        while context_start < len(sequence_ids) and sequence_ids[context_start] != 1:\n",
        "            context_start += 1\n",
        "        while context_end >= 0 and sequence_ids[context_end] != 1:\n",
        "            context_end -= 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Find the token start and end positions\n",
        "            token_start_index = context_start\n",
        "            while token_start_index <= context_end and offset[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            start_positions.append(token_start_index - 1)\n",
        "\n",
        "            token_end_index = context_end\n",
        "            while token_end_index >= context_start and offset[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            end_positions.append(token_end_index + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    inputs[\"sample_mapping\"] = sample_map  # Add it back with a different name\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing\n",
        "tokenized_validation = validation_dataset.map(preprocess_function, batched=True, remove_columns=validation_dataset.column_names)\n",
        "\n",
        "# Convert to PyTorch Tensors and create DataLoader\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "val_ids = torch.tensor(tokenized_validation['input_ids'])\n",
        "val_attention_mask = torch.tensor(tokenized_validation['attention_mask'])\n",
        "# We don't need labels for the inference itself, but we'll use them for evaluation later\n",
        "\n",
        "pin = torch.cuda.is_available()\n",
        "# Note: For QA, the DataLoader yields only input_ids and attention_mask.\n",
        "# The original examples are needed for evaluation.\n",
        "dataset = TensorDataset(val_ids, val_attention_mask)\n",
        "test_loader = DataLoader(dataset, batch_size=64, shuffle=False, pin_memory=pin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_omBO0cxf_3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install codecarbon --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI9keSOlAjjr"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3asMtO9xf_4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import codecarbon\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import evaluate # Using the new Hugging Face evaluate library\n",
        "from collections import defaultdict\n",
        "# --- Constants ---\n",
        "RESULTS_DIR = \"./test_results\"\n",
        "CARBON_DIR = \"./carbon_emissions\"\n",
        "RESULTS_FILE = os.path.join(RESULTS_DIR, \"qa_inference_metrics.csv\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(CARBON_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def simple_qa_inference(model, dataset, tokenizer, device, num_examples=None):\n",
        "    \"\"\"\n",
        "    Simple QA inference function that processes examples one by one\n",
        "    \"\"\"\n",
        "    if num_examples is None:\n",
        "        num_examples = len(dataset)\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "    batch_times = []\n",
        "    memory_usage = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(min(num_examples, len(dataset))), desc=\"QA Inference\"):\n",
        "            example = dataset[i]\n",
        "            question = example[\"question\"]\n",
        "            context = example[\"context\"]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True, max_length=384)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Time the inference\n",
        "            batch_start = time.time()\n",
        "            outputs = model(**inputs)\n",
        "            batch_times.append(time.time() - batch_start)\n",
        "\n",
        "            # Track memory usage\n",
        "            if torch.cuda.is_available():\n",
        "                memory_used = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "                memory_usage.append(memory_used)\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "            # Get most likely answer\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "            end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "            # Decode answer\n",
        "            input_ids = inputs[\"input_ids\"][0]\n",
        "            answer_tokens = input_ids[start_idx:end_idx+1]\n",
        "            answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "            predictions.append({\n",
        "                'id': example['id'],\n",
        "                'prediction_text': answer_text\n",
        "            })\n",
        "            references.append({\n",
        "                'id': example['id'],\n",
        "                'answers': example['answers']\n",
        "            })\n",
        "\n",
        "    return predictions, references, batch_times, memory_usage\n",
        "\n",
        "# --- Main Execution ---\n",
        "# Initialize the carbon tracker\n",
        "tracker = codecarbon.EmissionsTracker(output_dir=CARBON_DIR, project_name=\"QA_Inference\", log_level=\"critical\")\n",
        "tracker.start()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Run the simple inference\n",
        "predictions, references, batch_times, memory_usage = simple_qa_inference(\n",
        "    model=model,\n",
        "    dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    num_examples=len(validation_dataset)  # Adjust this number as needed\n",
        ")\n",
        "\n",
        "# Stop the tracker and get emissions\n",
        "emissions_kg = tracker.stop()\n",
        "\n",
        "# Compute metrics\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "qa_metrics = squad_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Calculate timing metrics\n",
        "total_inference_time = sum(batch_times)\n",
        "avg_batch_time = total_inference_time / len(batch_times) if batch_times else 0\n",
        "avg_memory_used = sum(memory_usage) / len(memory_usage) if memory_usage else 0\n",
        "\n",
        "# Compile all metrics\n",
        "metrics = {\n",
        "    'total_inference_time': total_inference_time,\n",
        "    'avg_batch_time': avg_batch_time,\n",
        "    'emissions_kg': emissions_kg,\n",
        "    'exact_match': qa_metrics['exact_match'],\n",
        "    'f1': qa_metrics['f1'],\n",
        "    'avg_memory_used_gb': avg_memory_used,\n",
        "    'num_examples_processed': len(predictions)\n",
        "}\n",
        "\n",
        "# Save metrics to CSV\n",
        "pd.DataFrame([metrics]).to_csv(RESULTS_FILE, index=False)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nðŸŽ¯ QA Inference Complete!\")\n",
        "print(f\"ðŸ“Š Number of examples processed: {metrics['num_examples_processed']}\")\n",
        "print(f\"â° Total Time: {metrics['total_inference_time']:.2f}s\")\n",
        "print(f\"ðŸ“¦ Average batch time: {metrics['avg_batch_time']:.4f}s\")\n",
        "print(f\"ðŸŒ± Emissions: {metrics['emissions_kg']:.6f} kg CO2\")\n",
        "print(f\"ðŸ’¾ Average memory used: {metrics['avg_memory_used_gb']:.2f} GB\")\n",
        "print(f\"ðŸŽ¯ Exact Match: {metrics['exact_match']:.4f}\")\n",
        "print(f\"ðŸ“ˆ F1 Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ“ Results saved to: {RESULTS_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ggv4N-MM9VX"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAUdyQYQB41N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"/content/test_results/qa_inference_metrics.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF1QL2Y-QcL2"
      },
      "outputs": [],
      "source": [
        "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
        "\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "inputs.to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_attentions=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:10.413622Z",
          "iopub.status.busy": "2025-10-07T17:16:10.413289Z",
          "iopub.status.idle": "2025-10-07T17:16:10.417268Z",
          "shell.execute_reply": "2025-10-07T17:16:10.416550Z",
          "shell.execute_reply.started": "2025-10-07T17:16:10.413595Z"
        },
        "id": "5m4o_Yzqxf_6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "attentions=outputs.attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:10.420459Z",
          "iopub.status.busy": "2025-10-07T17:16:10.420265Z",
          "iopub.status.idle": "2025-10-07T17:16:21.777137Z",
          "shell.execute_reply": "2025-10-07T17:16:21.776330Z",
          "shell.execute_reply.started": "2025-10-07T17:16:10.420445Z"
        },
        "id": "reVAWUWwxf_6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#hist layer vise\n",
        "num_layers = len(attentions)\n",
        "num_heads = attentions[0].size(1)\n",
        "\n",
        "fig, axes = plt.subplots(num_layers, num_heads, figsize=(20, 15))\n",
        "\n",
        "for layer_idx, layer_attn in enumerate(attentions):\n",
        "    # Take first example from batch (instead of averaging)\n",
        "    attn = layer_attn[0]  # shape: [num_heads, seq_len, seq_len]\n",
        "\n",
        "    for head_idx in range(num_heads):\n",
        "        values = attn[head_idx].detach().cpu().numpy().flatten()\n",
        "\n",
        "        ax = axes[layer_idx, head_idx] if num_layers > 1 else axes[head_idx]\n",
        "        ax.hist(values, bins=50)  # zoom in on [0, 0.3]\n",
        "        ax.set_title(f\"Layer:{layer_idx} Head:{head_idx}\")\n",
        "        ax.set_xlabel(\"Attention weight\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:21.778098Z",
          "iopub.status.busy": "2025-10-07T17:16:21.777887Z",
          "iopub.status.idle": "2025-10-07T17:16:25.175344Z",
          "shell.execute_reply": "2025-10-07T17:16:25.174557Z",
          "shell.execute_reply.started": "2025-10-07T17:16:21.778082Z"
        },
        "id": "Mu9Vprmoxf_7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install bertviz --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:25.176615Z",
          "iopub.status.busy": "2025-10-07T17:16:25.176339Z",
          "iopub.status.idle": "2025-10-07T17:16:25.185962Z",
          "shell.execute_reply": "2025-10-07T17:16:25.185222Z",
          "shell.execute_reply.started": "2025-10-07T17:16:25.176585Z"
        },
        "id": "Hpx3o7k1xf_8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input = inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:25.186990Z",
          "iopub.status.busy": "2025-10-07T17:16:25.186733Z",
          "iopub.status.idle": "2025-10-07T17:16:25.197356Z",
          "shell.execute_reply": "2025-10-07T17:16:25.196580Z",
          "shell.execute_reply.started": "2025-10-07T17:16:25.186969Z"
        },
        "id": "-tUci0plxf_8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Convert all sequences in batch\n",
        "all_tokens = [\n",
        "    tokenizer.convert_ids_to_tokens(seq.tolist())\n",
        "    for seq in input[\"input_ids\"]\n",
        "]\n",
        "\n",
        "for i, tokens in enumerate(all_tokens):\n",
        "    print(f\"Sequence {i} tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:25.198700Z",
          "iopub.status.busy": "2025-10-07T17:16:25.198235Z",
          "iopub.status.idle": "2025-10-07T17:16:25.420121Z",
          "shell.execute_reply": "2025-10-07T17:16:25.419290Z",
          "shell.execute_reply.started": "2025-10-07T17:16:25.198683Z"
        },
        "id": "cfZh8cM-xf_8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from bertviz import head_view\n",
        "\n",
        "# choose the first sequence in batch\n",
        "seq_index = 0\n",
        "\n",
        "# stack attentions across layers into [num_layers, num_heads, seq, seq]\n",
        "attention_stack = torch.stack([layer[seq_index] for layer in attentions])\n",
        "\n",
        "# add batch dimension â†’ [num_layers, 1, num_heads, seq, seq]\n",
        "attention_stack = attention_stack.unsqueeze(1)\n",
        "\n",
        "# tokens for that sequence\n",
        "tokens = all_tokens[seq_index]\n",
        "\n",
        "# visualize\n",
        "head_view(attention_stack, tokens, sentence_b_start=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:25.421486Z",
          "iopub.status.busy": "2025-10-07T17:16:25.421254Z",
          "iopub.status.idle": "2025-10-07T17:16:26.027426Z",
          "shell.execute_reply": "2025-10-07T17:16:26.026774Z",
          "shell.execute_reply.started": "2025-10-07T17:16:25.421469Z"
        },
        "id": "B-FIBTcBxf_9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "layer, head = 1,1\n",
        "weights = attention_stack[layer, 0, head].detach().cpu()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(weights, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "plt.title(f\"Layer {layer}, Head {head}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:26.028589Z",
          "iopub.status.busy": "2025-10-07T17:16:26.028350Z",
          "iopub.status.idle": "2025-10-07T17:16:26.222413Z",
          "shell.execute_reply": "2025-10-07T17:16:26.221238Z",
          "shell.execute_reply.started": "2025-10-07T17:16:26.028571Z"
        },
        "id": "YRXgFWunxf_9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from bertviz import model_view\n",
        "model_view(attention_stack,tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:26.224531Z",
          "iopub.status.busy": "2025-10-07T17:16:26.224100Z",
          "iopub.status.idle": "2025-10-07T17:16:26.253144Z",
          "shell.execute_reply": "2025-10-07T17:16:26.252281Z",
          "shell.execute_reply.started": "2025-10-07T17:16:26.224490Z"
        },
        "id": "Mqkwd8_Uxf_-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List\n",
        "\n",
        "# ---------- 1) tokens for all sequences ----------\n",
        "def tokens_for_all(inputs, tokenizer):\n",
        "    all_token_lists = []\n",
        "    ids = inputs[\"input_ids\"]  # shape [batch, seq_len]\n",
        "    for seq in ids:\n",
        "        seq_list = seq.tolist()\n",
        "        tokens = tokenizer.convert_ids_to_tokens(seq_list)\n",
        "        all_token_lists.append(tokens)\n",
        "    return all_token_lists\n",
        "\n",
        "# ---------- 2) heatmap for a single layer/head ----------\n",
        "def plot_attention_heatmap(attentions, inputs, tokenizer, layer=0, head=0, seq_index=0, figsize=(8,6), cmap='viridis'):\n",
        "    attn = attentions[layer][seq_index, head]   # shape [seq_len, seq_len]\n",
        "    attn_np = attn.detach().cpu().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][seq_index].tolist())\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(attn_np, xticklabels=tokens, yticklabels=tokens, square=True, cbar=True, cmap=cmap)\n",
        "    plt.title(f\"Layer {layer} Head {head} (seq {seq_index})\")\n",
        "    plt.xlabel(\"Key tokens\")\n",
        "    plt.ylabel(\"Query tokens\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------- 3) grid of head heatmaps for one layer ----------\n",
        "def plot_layer_heads(attentions, inputs, tokenizer, layer=0, seq_index=0, max_heads=12, figsize_per_plot=(3,3)):\n",
        "    heads = attentions[layer].size(1)\n",
        "    heads = min(heads, max_heads)\n",
        "    cols = min(6, heads)\n",
        "    rows = int(np.ceil(heads / cols))\n",
        "    fig_w = cols * figsize_per_plot[0]\n",
        "    fig_h = rows * figsize_per_plot[1]\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(fig_w, fig_h))\n",
        "    axes = axes.flatten()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][seq_index].tolist())\n",
        "\n",
        "    for h in range(heads):\n",
        "        ax = axes[h]\n",
        "        attn_np = attentions[layer][seq_index, h].detach().cpu().numpy()\n",
        "        sns.heatmap(attn_np, ax=ax, cbar=False, xticklabels=tokens, yticklabels=tokens)\n",
        "        ax.set_title(f\"Layer:{layer} Head:{h}\")\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    for i in range(heads, len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------- 4) histogram distributions per head ----------\n",
        "def plot_histograms(attentions, seq_index=0, bins=50, value_range=(0,0.3)):\n",
        "    num_layers = len(attentions)\n",
        "    num_heads = attentions[0].size(1)\n",
        "    fig, axes = plt.subplots(num_layers, num_heads, figsize=(num_heads*2, num_layers*1.8))\n",
        "    for l, layer_attn in enumerate(attentions):\n",
        "        attn = layer_attn[seq_index]  # [heads, seq_len, seq_len]\n",
        "        for h in range(num_heads):\n",
        "            ax = axes[l, h] if num_layers>1 else axes[h]\n",
        "            vals = attn[h].detach().cpu().numpy().flatten()\n",
        "            ax.hist(vals, bins=bins, range=value_range)\n",
        "            ax.set_title(f\"Layer:{l} Head:{h}\")\n",
        "            ax.set_xlim(value_range)\n",
        "            ax.set_ylim(0, None)\n",
        "            ax.set_xticks([value_range[0], value_range[1]])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------- 5) attention rollout ----------\n",
        "def attention_rollout(attentions, discard_ratio=0.0):\n",
        "    avg_per_layer = [layer_attn.mean(1) for layer_attn in attentions]  # [batch, seq, seq]\n",
        "    batch = avg_per_layer[0].size(0)\n",
        "    seq_len = avg_per_layer[0].size(1)\n",
        "    device = avg_per_layer[0].device  # FIXED DEVICE\n",
        "    rollouts = []\n",
        "    for b in range(batch):\n",
        "        joint = torch.eye(seq_len, device=device)  # FIXED DEVICE\n",
        "        for L in avg_per_layer:\n",
        "            A = L[b]\n",
        "            if discard_ratio > 0:\n",
        "                flat = A.flatten()\n",
        "                thresh = torch.quantile(flat, discard_ratio)\n",
        "                A = torch.where(A < thresh, torch.zeros_like(A), A)\n",
        "            A = A + torch.eye(seq_len, device=A.device)  # FIXED DEVICE\n",
        "            A = A / A.sum(dim=-1, keepdim=True)\n",
        "            joint = A @ joint\n",
        "        rollouts.append(joint.detach().cpu().numpy())\n",
        "    return rollouts\n",
        "\n",
        "def plot_rollout(rollouts, inputs, tokenizer, seq_index=0):\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][seq_index].tolist())\n",
        "    mat = rollouts[seq_index]\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(mat, xticklabels=tokens, yticklabels=tokens, square=True, cbar=True)\n",
        "    plt.title(\"Attention rollout (token influence)\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------- 6) head similarity matrix ----------\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def head_similarity_matrix(attentions, layer, seq_index=0):\n",
        "    layer_attn = attentions[layer][seq_index]  # [heads, seq, seq]\n",
        "    H = layer_attn.size(0)\n",
        "    patterns = []\n",
        "    for h in range(H):\n",
        "        pat = layer_attn[h].detach().cpu().numpy().flatten()\n",
        "        patterns.append(pat)\n",
        "    sim = cosine_similarity(patterns)\n",
        "    return sim\n",
        "\n",
        "def plot_head_similarity(sim, layer):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(sim, annot=True, fmt=\".2f\")\n",
        "    plt.title(f\"Head similarity (Layer {layer})\")\n",
        "    plt.xlabel(\"Head\")\n",
        "    plt.ylabel(\"Head\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------- 7) per-head CLS mass ----------\n",
        "def cls_mass_per_head(attentions, inputs, cls_index=0, seq_index=0):\n",
        "    masses = []\n",
        "    for l, layer_attn in enumerate(attentions):\n",
        "        a = layer_attn[seq_index]  # [heads, seq, seq]\n",
        "        mass_to_cls = a[:, :, cls_index].sum(axis=1).detach().cpu().numpy()\n",
        "        masses.append(mass_to_cls)\n",
        "    return np.stack(masses, axis=0)  # [layers, heads]\n",
        "\n",
        "def plot_cls_mass(mass_matrix):\n",
        "    layers, heads = mass_matrix.shape\n",
        "    plt.figure(figsize=(heads*0.5 + 3, layers*0.4 + 2))\n",
        "    sns.heatmap(mass_matrix, annot=False, cmap='magma')\n",
        "    plt.xlabel(\"Head\")\n",
        "    plt.ylabel(\"Layer\")\n",
        "    plt.title(\"Per-head mass attending TO CLS (rows=layers)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:26.255551Z",
          "iopub.status.busy": "2025-10-07T17:16:26.254612Z",
          "iopub.status.idle": "2025-10-07T17:16:26.913402Z",
          "shell.execute_reply": "2025-10-07T17:16:26.912685Z",
          "shell.execute_reply.started": "2025-10-07T17:16:26.255525Z"
        },
        "id": "8xazzTYKxgAA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# assume tokenizer, model loaded; inputs tokenized; outputs computed\n",
        "output = outputs\n",
        "attentions = output.attentions\n",
        "all_tokens = tokens_for_all(input, tokenizer)\n",
        "print(\"batch size:\", len(all_tokens))\n",
        "\n",
        "# Plot a single heatmap\n",
        "plot_attention_heatmap(attentions, input, tokenizer, layer=0, head=0, seq_index=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:26.914762Z",
          "iopub.status.busy": "2025-10-07T17:16:26.914465Z",
          "iopub.status.idle": "2025-10-07T17:16:31.460176Z",
          "shell.execute_reply": "2025-10-07T17:16:31.459250Z",
          "shell.execute_reply.started": "2025-10-07T17:16:26.914739Z"
        },
        "id": "XsPhdW51xgAB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Plot grid of heads for layer 0, example 0\n",
        "plot_layer_heads(attentions, input, tokenizer, layer=0, seq_index=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:31.461215Z",
          "iopub.status.busy": "2025-10-07T17:16:31.460995Z",
          "iopub.status.idle": "2025-10-07T17:16:32.071080Z",
          "shell.execute_reply": "2025-10-07T17:16:32.070419Z",
          "shell.execute_reply.started": "2025-10-07T17:16:31.461199Z"
        },
        "id": "Y7UVzAHoxgAC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Attention rollout and plot\n",
        "rollouts = attention_rollout(attentions, discard_ratio=0.0)\n",
        "plot_rollout(rollouts, input,tokenizer=tokenizer, seq_index=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:32.071981Z",
          "iopub.status.busy": "2025-10-07T17:16:32.071792Z",
          "iopub.status.idle": "2025-10-07T17:16:32.524550Z",
          "shell.execute_reply": "2025-10-07T17:16:32.523810Z",
          "shell.execute_reply.started": "2025-10-07T17:16:32.071967Z"
        },
        "id": "H-u6Mm-BxgAC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Head similarity & visualization\n",
        "sim = head_similarity_matrix(attentions, layer=0, seq_index=0)\n",
        "plot_head_similarity(sim, layer=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:32.525680Z",
          "iopub.status.busy": "2025-10-07T17:16:32.525461Z",
          "iopub.status.idle": "2025-10-07T17:16:32.769623Z",
          "shell.execute_reply": "2025-10-07T17:16:32.768744Z",
          "shell.execute_reply.started": "2025-10-07T17:16:32.525663Z"
        },
        "id": "3SoekPVQxgAD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# CLS mass across layers\n",
        "mass = cls_mass_per_head(attentions, input, cls_index=0, seq_index=0)\n",
        "plot_cls_mass(mass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:32.770769Z",
          "iopub.status.busy": "2025-10-07T17:16:32.770525Z",
          "iopub.status.idle": "2025-10-07T17:16:35.979813Z",
          "shell.execute_reply": "2025-10-07T17:16:35.978854Z",
          "shell.execute_reply.started": "2025-10-07T17:16:32.770747Z"
        },
        "id": "JK0_y0ajxgAD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install captum --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:37.783585Z",
          "iopub.status.busy": "2025-10-07T17:16:37.783373Z",
          "iopub.status.idle": "2025-10-07T17:16:39.042640Z",
          "shell.execute_reply": "2025-10-07T17:16:39.041914Z",
          "shell.execute_reply.started": "2025-10-07T17:16:37.783569Z"
        },
        "id": "7_1YDaESxgAH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, layer_attention in enumerate(attentions):\n",
        "    # Get attention for the first (and only) item in the batch\n",
        "    # and average across all heads: shape becomes [seq_len, seq_len]\n",
        "    layer_attention_avg = layer_attention[0].mean(dim=0)\n",
        "\n",
        "    # Flatten the matrix to get a 1D array of weights\n",
        "    weights = layer_attention_avg.detach().cpu().numpy().flatten()\n",
        "\n",
        "    ax = axes[i]\n",
        "    ax.hist(weights, bins=50, color='skyblue', edgecolor='black')\n",
        "    ax.set_title(f'Layer {i}: Weight Distribution')\n",
        "    ax.set_xlabel('Attention Weight')\n",
        "    ax.set_ylabel('Frequency')\n",
        "\n",
        "plt.suptitle('Distribution of Attention Weights Across Layers (Averaged over Heads)', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:39.046358Z",
          "iopub.status.busy": "2025-10-07T17:16:39.046134Z",
          "iopub.status.idle": "2025-10-07T17:16:39.676578Z",
          "shell.execute_reply": "2025-10-07T17:16:39.675920Z",
          "shell.execute_reply.started": "2025-10-07T17:16:39.046341Z"
        },
        "id": "7OuD-lNjxgAI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# --- You can change these values to explore different layers/heads ---\n",
        "LAYER_TO_VISUALIZE = 4\n",
        "HEAD_TO_VISUALIZE = 8\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Get attention weights for the specific layer and head\n",
        "attention_weights = attentions[LAYER_TO_VISUALIZE][0, HEAD_TO_VISUALIZE].detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(attention_weights, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "plt.title(f'Attention Heatmap for Layer {LAYER_TO_VISUALIZE}, Head {HEAD_TO_VISUALIZE}')\n",
        "plt.xlabel(\"Key Tokens (attended to)\")\n",
        "plt.ylabel(\"Query Tokens (attending from)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-07T17:16:39.677804Z",
          "iopub.status.busy": "2025-10-07T17:16:39.677474Z",
          "iopub.status.idle": "2025-10-07T17:16:48.075414Z",
          "shell.execute_reply": "2025-10-07T17:16:48.074615Z",
          "shell.execute_reply.started": "2025-10-07T17:16:39.677773Z"
        },
        "id": "HDI0We6hxgAJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# --- Plot 1 (Revised): Histogram for Each Head in Each Layer ---\n",
        "num_layers = len(attentions)\n",
        "num_heads = attentions[0].size(1) # Number of heads in the first layer\n",
        "\n",
        "fig, axes = plt.subplots(num_layers, num_heads, figsize=(20, 10))\n",
        "\n",
        "for layer_idx, layer_attention in enumerate(attentions):\n",
        "    # Get attention for the first (and only) item in the batch\n",
        "    attention_for_seq = layer_attention[0] # Shape: [num_heads, seq_len, seq_len]\n",
        "\n",
        "    for head_idx in range(num_heads):\n",
        "        # Get the specific head's attention matrix and flatten it\n",
        "        head_weights = attention_for_seq[head_idx].detach().cpu().numpy().flatten()\n",
        "\n",
        "        ax = axes[layer_idx, head_idx]\n",
        "        ax.hist(head_weights, bins=30, color='steelblue', edgecolor='black')\n",
        "        ax.set_title(f'L{layer_idx} H{head_idx}', fontsize=8)\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_xticklabels([])\n",
        "\n",
        "plt.suptitle('Distribution of Attention Weights for Each Head Across Layers', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg1owsc8a0zz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers-interpret --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKQB8aiBcLdQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers_interpret import QuestionAnsweringExplainer\n",
        "\n",
        "# --- 1. Load your model and tokenizer (same as before) ---\n",
        "model_checkpoint = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# --- 2. Define your example data ---\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"Paris is the capital and most populous city of France, with an estimated population of 2,165,423 residents as of 2022.\"\n",
        "\n",
        "# --- 3. Get the model's prediction (we need to know what to explain) ---\n",
        "inputs = tokenizer(question, context, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "start_idx = torch.argmax(outputs.start_logits)\n",
        "end_idx = torch.argmax(outputs.end_logits)\n",
        "predicted_answer_tokens = inputs.input_ids[0, start_idx : end_idx + 1]\n",
        "predicted_answer = tokenizer.decode(predicted_answer_tokens)\n",
        "\n",
        "# --- 4. Use the Explainer ---\n",
        "# Initialize the explainer for Question Answering\n",
        "qa_explainer = QuestionAnsweringExplainer(model, tokenizer)\n",
        "\n",
        "# Get attributions (saliency scores) for the predicted answer\n",
        "word_attributions = qa_explainer(question, context, predicted_answer)\n",
        "\n",
        "# The library comes with a powerful, built-in visualizer!\n",
        "qa_explainer.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNHTyPNDdPfG"
      },
      "outputs": [],
      "source": [
        "from captum.attr import Saliency, IntegratedGradients\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Where did Sandra Day grow up?\",\n",
        "        \"context\": \"Sandra Day O'Connor was born in El Paso, Texas, and grew up on a ranch in Duncan, Arizona.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the taj mahal made of?\",\n",
        "        \"context\": \"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1632 by the Mughal emperor Shah Jahan.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the symptoms of COVID-19?\",\n",
        "        \"context\": \"Common symptoms of COVID-19 include fever, tiredness, and dry cough. Some patients may have aches and pains.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- 3. Captum Wrapper Functions (same as before) ---\n",
        "embedding_layer = model.distilbert.embeddings.word_embeddings\n",
        "\n",
        "def saliency_forward_start(inputs_embeds, attention_mask):\n",
        "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask.to(device))\n",
        "    return outputs.start_logits\n",
        "\n",
        "def saliency_forward_end(inputs_embeds, attention_mask):\n",
        "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask.to(device))\n",
        "    return outputs.end_logits\n",
        "\n",
        "# --- 4. Prediction and Attribution Calculation Function ---\n",
        "def explain_qa_example(question, context, explainer_algo=Saliency):\n",
        "    \"\"\"\n",
        "    Performs QA prediction and calculates combined saliency scores for an example.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()} # Move inputs to device\n",
        "\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    start_idx = torch.argmax(outputs.start_logits)\n",
        "    end_idx = torch.argmax(outputs.end_logits)\n",
        "\n",
        "    predicted_answer_tokens = inputs['input_ids'][0, start_idx : end_idx + 1]\n",
        "    predicted_answer = tokenizer.decode(predicted_answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Captum attributions\n",
        "    # Make sure to pass a copy of the embedding, and require_grad=True\n",
        "    input_embeds = embedding_layer(inputs['input_ids']).requires_grad_(True)\n",
        "\n",
        "    # Instantiate the explainer for start and end logits\n",
        "    attr_start_explainer = explainer_algo(saliency_forward_start)\n",
        "    attr_end_explainer = explainer_algo(saliency_forward_end)\n",
        "\n",
        "    # Calculate attributions\n",
        "    attrs_start = attr_start_explainer.attribute(\n",
        "        inputs=input_embeds,\n",
        "        target=start_idx,\n",
        "        additional_forward_args=(inputs['attention_mask'],) # pass attention_mask as additional arg\n",
        "    )\n",
        "    attrs_end = attr_end_explainer.attribute(\n",
        "        inputs=input_embeds,\n",
        "        target=end_idx,\n",
        "        additional_forward_args=(inputs['attention_mask'],)\n",
        "    )\n",
        "\n",
        "    # Summarize attributions: take the L2 norm over the embedding dimension\n",
        "    attrs_start_sum = attrs_start.norm(p=2, dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "    attrs_end_sum = attrs_end.norm(p=2, dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "    # Combine the start and end scores (e.g., by summing them)\n",
        "    # You might also experiment with taking the max or a weighted sum\n",
        "    combined_saliency_scores = attrs_start_sum + attrs_end_sum\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().cpu())\n",
        "\n",
        "    return tokens, combined_saliency_scores, predicted_answer\n",
        "\n",
        "# --- 5. Custom Plotting Function (similar to your example) ---\n",
        "def plot_word_attribution(tokens, scores, question, predicted_answer, ax):\n",
        "    \"\"\"\n",
        "    Plots word attribution scores as a bar chart.\n",
        "    \"\"\"\n",
        "    # Normalize scores to 0-1 for consistent bar heights\n",
        "    max_score = np.max(scores)\n",
        "    if max_score == 0: # Avoid division by zero if all scores are 0\n",
        "        normalized_scores = np.zeros_like(scores)\n",
        "    else:\n",
        "        normalized_scores = scores / max_score\n",
        "\n",
        "    x_pos = np.arange(len(tokens))\n",
        "\n",
        "    # Plotting bars (blue with red outline, like your example)\n",
        "    ax.bar(x_pos, normalized_scores, width=0.8, color='blue', alpha=0.6, edgecolor='red', linewidth=0.8)\n",
        "\n",
        "    # Customize ticks and labels\n",
        "    ax.set_xticks(x_pos)\n",
        "    ax.set_xticklabels(tokens, rotation=90, fontsize=8) # Rotate labels for readability\n",
        "    ax.set_ylim(0, 1) # Scores are normalized 0-1\n",
        "    ax.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "    # Set title with question and predicted answer\n",
        "    ax.set_title(f\"Q: {question} | A: {predicted_answer}\", fontsize=10)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# --- Main Execution for multiple examples ---\n",
        "fig, axes = plt.subplots(nrows=len(examples), figsize=(16, 5 * len(examples)))\n",
        "if len(examples) == 1: # If only one example, axes is not an array\n",
        "    axes = [axes]\n",
        "\n",
        "for i, example in enumerate(examples):\n",
        "    tokens, saliency_scores, predicted_answer = explain_qa_example(\n",
        "        example[\"question\"], example[\"context\"], explainer_algo=IntegratedGradients # Choose Saliency or IntegratedGradients\n",
        "    )\n",
        "    plot_word_attribution(tokens, saliency_scores, example[\"question\"], predicted_answer, axes[i])\n",
        "# plt.suptitle(\"Lev Attention Word Attribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8191573,
          "sourceId": 12944351,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8191809,
          "sourceId": 12944728,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
